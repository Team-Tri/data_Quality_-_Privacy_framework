{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from src.Utils.HelperFunction.helper import fetchDataMssql,encrypt_data,generate_deterministic_key,generate_non_deterministic_key\n",
    "from src.Utils.connectors.connector import connectMssql\n",
    "import json\n",
    "import re ,ast\n",
    "from src.Utils.anonymiser.anonymiser import anonymize\n",
    "from Utils.sparksession.local_spark_utility import get_mssql_sparkContext\n",
    "from Utils.dq_utils.table_level_intial_assessment import get_tablelevel_asessment_stats\n",
    "from Utils.dq_utils.column_level_intial_assessment import get_columnlevel_assessment_stats\n",
    "import numpy as np\n",
    "from src.Utils.GenAiClassifier.genaiPIIClassifier import getClassifiedDf\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\jars\\mssql-jdbc-12.8.1.jre11.jar,C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\jars\\mysql-connector-java-5.1.49.jar,C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\jars\\spark-mssql-connector_2.12-1.2.0.jar,C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\jars\\spark-mssql-connector_2.12-1.4.0-BETA.jar\n"
     ]
    }
   ],
   "source": [
    "spark = get_mssql_sparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"\\Users\\dhanush.shetty\\DContracts_DQP\\src\\config\\config.json\",\"r\") as j:\n",
    "    conf_=json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_= pd.read_csv(r\"C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\access_control\\Access_table.csv\")\n",
    "  \n",
    "  #asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\metadata\\Metadata_mssql_adventureworks.csv\"\n",
    "key=\"AIzaSyAkrpur0_GaH0LRupezuaHvAZY5NlusWBU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Data privacy Steward ,You have to help me tag below given Columns With                         PII Tags ,provide answer in columnar manner with PII tag As 2nd column , Limit                          value to PII column Strictly to Quasi/Yes/No where Quasi is not PII but Quasi-Identifier Data.                                                                 response would be used in automation,strict instruction for response dont send anything else than csv format (dont truncate any data ,need all values),no explanation or notes needed                            Below is the full column names                                                                          0      DATABASELOG.DATABASELOGID\n",
      "1           DATABASELOG.POSTTIME\n",
      "2       DATABASELOG.DATABASEUSER\n",
      "3              DATABASELOG.EVENT\n",
      "4             DATABASELOG.SCHEMA\n",
      "                 ...            \n",
      "354             SYSDIAGRAMS.NAME\n",
      "355     SYSDIAGRAMS.PRINCIPAL_ID\n",
      "356       SYSDIAGRAMS.DIAGRAM_ID\n",
      "357          SYSDIAGRAMS.VERSION\n",
      "358       SYSDIAGRAMS.DEFINITION\n",
      "Name: full_column_name, Length: 359, dtype: object                                                                                               Also add a Column named Compliance ,value in compliance column should be in single value recommendation                         for which compliance to follow Example like Hippa,GDPR etc                                                  Add 4th column which gives suggestion for what type of encryption to be done on given                         column ,for columns with no suggestion fill with 'Not Applicable                                             Format for output : [column_name,pii,compliance_to_follow,Recommended_Masking]                         Add 5th Column named TAG which gives what domain column is choose from [Person,Date,Location,Email,Phone,Card]                         for any other domain not in this list just insert 'Sensitive' if its senstive column else 'None'.                         For example columns related Person names would be person,Address would be location                        column ,for columns with no suggestion fill with 'Not Applicable'\n",
      "```csv\n",
      "column_name,pii,compliance_to_follow,Recommended_Masking,TAG\n",
      "DATABASELOG.DATABASELOGID,No,GDPR,Not Applicable,None\n",
      "DATABASELOG.POSTTIME,Quasi,GDPR,Not Applicable,Date\n",
      "DATABASELOG.DATABASEUSER,Quasi,GDPR,Pseudonymization,Person\n",
      "DATABASELOG.EVENT,No,GDPR,Not Applicable,Sensitive\n",
      "DATABASELOG.SCHEMA,No,GDPR,Not Applicable,None\n",
      "SYSDIAGRAMS.NAME,No,GDPR,Not Applicable,None\n",
      "SYSDIAGRAMS.PRINCIPAL_ID,Quasi,GDPR,Pseudonymization,Person\n",
      "SYSDIAGRAMS.DIAGRAM_ID,No,GDPR,Not Applicable,None\n",
      "SYSDIAGRAMS.VERSION,No,GDPR,Not Applicable,None\n",
      "SYSDIAGRAMS.DEFINITION,No,GDPR,Not Applicable,None\n",
      "```\n",
      "\n",
      "['DATABASELOG.SCHEMA,No,GDPR,Not Applicable,None', 'SYSDIAGRAMS.NAME,No,GDPR,Not Applicable,None', 'SYSDIAGRAMS.PRINCIPAL_ID,Quasi,GDPR,Pseudonymization,Person', 'SYSDIAGRAMS.DIAGRAM_ID,No,GDPR,Not Applicable,None']\n"
     ]
    }
   ],
   "source": [
    "classified_df,_=getClassifiedDf(key,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Column_Name</th>\n",
       "      <th>Pii</th>\n",
       "      <th>Compliance_to_follow</th>\n",
       "      <th>Recommended_Encryption</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_name</td>\n",
       "      <td>pii</td>\n",
       "      <td>compliance_to_follow</td>\n",
       "      <td>Recommended_Masking</td>\n",
       "      <td>TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATABASELOG.DATABASELOGID</td>\n",
       "      <td>No</td>\n",
       "      <td>GDPR</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DATABASELOG.POSTTIME</td>\n",
       "      <td>Quasi</td>\n",
       "      <td>GDPR</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATABASELOG.DATABASEUSER</td>\n",
       "      <td>Quasi</td>\n",
       "      <td>GDPR</td>\n",
       "      <td>Pseudonymization</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DATABASELOG.EVENT</td>\n",
       "      <td>No</td>\n",
       "      <td>GDPR</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Full_Column_Name    Pii  Compliance_to_follow  \\\n",
       "0                column_name    pii  compliance_to_follow   \n",
       "1  DATABASELOG.DATABASELOGID     No                  GDPR   \n",
       "2       DATABASELOG.POSTTIME  Quasi                  GDPR   \n",
       "3   DATABASELOG.DATABASEUSER  Quasi                  GDPR   \n",
       "4          DATABASELOG.EVENT     No                  GDPR   \n",
       "\n",
       "  Recommended_Encryption        Tag  \n",
       "0    Recommended_Masking        TAG  \n",
       "1         Not Applicable       None  \n",
       "2         Not Applicable       Date  \n",
       "3       Pseudonymization     Person  \n",
       "4         Not Applicable  Sensitive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert classified metadata df to spark df\n",
    "classified_df_spark = spark.createDataFrame(classified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o68.load.\n: java.lang.UnsupportedClassVersionError: com/microsoft/sqlserver/jdbc/SQLServerDriver has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_ \u001b[38;5;129;01min\u001b[39;00m conf_[user_]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTABLE_REQUIRED\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(access_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER_ACCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m][access_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable Name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mtable_]\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m#fetch data into df\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m#save data to data_df variable\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m#run table level score\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         data_df, table_assessment_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_tablelevel_asessment_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATABASE_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase_ip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATABASE_IP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassified_df_spark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m#run column level score\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         column_assessment_list \u001b[38;5;241m=\u001b[39m get_columnlevel_assessment_stats(spark\u001b[38;5;241m=\u001b[39mspark, table_name\u001b[38;5;241m=\u001b[39mtable_, database_name\u001b[38;5;241m=\u001b[39mDATABASE_, database_ip\u001b[38;5;241m=\u001b[39mDATABASE_IP, metadata_df\u001b[38;5;241m=\u001b[39mclassified_df_spark)\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\Utils\\dq_utils\\table_level_intial_assessment.py:24\u001b[0m, in \u001b[0;36mget_tablelevel_asessment_stats\u001b[1;34m(spark, table_name, database_name, database_ip, metadata_df)\u001b[0m\n\u001b[0;32m     21\u001b[0m freshness_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     23\u001b[0m metadata_df_mssql \u001b[38;5;241m=\u001b[39m metadata_df\n\u001b[1;32m---> 24\u001b[0m original_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_table_from_mssql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_ip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase_ip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m integrity_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Calculating Referential integrity score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\Utils\\connectors\\spark_connectors.py:8\u001b[0m, in \u001b[0;36mget_table_from_mssql\u001b[1;34m(spark, table_name, db_ip, db_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m jdbc_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:sqlserver://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_ip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;databaseName=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;integratedSecurity=true;trustServerCertificate=true\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\pyspark\\sql\\readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o68.load.\n: java.lang.UnsupportedClassVersionError: com/microsoft/sqlserver/jdbc/SQLServerDriver has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
     ]
    }
   ],
   "source": [
    "for user_ in conf_:\n",
    "    SERVER_=conf_[user_].get(\"SERVER_NAME\")\n",
    "    DATABASE_=conf_[user_].get(\"DATABASE_NAME\")\n",
    "    DATABASE_IP=conf_[user_].get(\"DATABASE_IP\")\n",
    "    SCHEMA_=conf_[user_].get(\"SCHEMA_NAME\")\n",
    "    PASSWORD_=conf_[user_].get(\"PASSWORD\")\n",
    "    #get tables\n",
    "    dq_table_lst=[]\n",
    "    for table_ in conf_[user_].get(\"TABLE_REQUIRED\"):\n",
    "        \n",
    "        if user_ in str(access_[\"USER_ACCESS\"][access_[\"Table Name\"]==table_].values):\n",
    "            #fetch data into df\n",
    "            #save data to data_df variable\n",
    "            #run table level score\n",
    "            data_df, table_assessment_dict = get_tablelevel_asessment_stats(spark=spark, table_name=table_, database_name=DATABASE_, database_ip=DATABASE_IP, metadata_df=classified_df_spark)\n",
    "            data_df = data_df.toPandas()\n",
    "            #run column level score\n",
    "            column_assessment_list = get_columnlevel_assessment_stats(spark=spark, table_name=table_, database_name=DATABASE_, database_ip=DATABASE_IP, metadata_df=classified_df_spark)\n",
    "            columns_to_retain = conf_[user_][\"Columns_to_retain\"]\n",
    "            columns_to_discard = conf_[user_][\"Columns_to_discard\"]\n",
    "            columns_to_custom_anonymise = list(conf_[user_][\"Columns_for_custom_anonymise\"].keys())\n",
    "            pd.DataFrame(column_assessment_list).to_csv(f\"{user_}_{table_}_column_quality.csv\")\n",
    "            #iterate column and tag from classified data\n",
    "            dq_table_lst.append(table_assessment_dict)\n",
    "            for column_,tag_ in classified_df[[\"Column Name\",\"Tag\"]][((classified_df[\"Table Name\"]==table_)\\\n",
    "                                                                     & \\\n",
    "                                                                       ((classified_df[\"Pii\"]==\"Yes\") \\\n",
    "                                                                        | (classified_df[\"Column Name\"].isin(columns_to_retain) \\\n",
    "                                                                        | (classified_df[\"Column Name\"].isin(columns_to_custom_anonymise) )))\\\n",
    "                                                                     & \\\n",
    "                                                                        (~classified_df[\"Column Name\"].isin(columns_to_discard)))\\\n",
    "                                                                    ].values:\n",
    "                \n",
    "                    #seperate rules from config \n",
    "                    if column_ not in  columns_to_custom_anonymise :\n",
    "                        #skip column with no tags\n",
    "                        if str(tag_) != \"nan\":\n",
    "                            rule_ = conf_[user_].get(f\"{tag_}\")\n",
    "                            #debug for checking values\n",
    "                            # print(user_,column_,tag_,rule_)\n",
    "                            #check if rules if encryption is asked to register keys and user into log for security purpose\n",
    "                            if [x for x in rule_.keys()][0] == \"encrypt\":\n",
    "                                # check if its deterministic or non deterministic type of encryption as both requires different keys\n",
    "                                #debug\n",
    "                                # print(\"yes\")\n",
    "                                if rule_[\"encrypt\"] == \"Deterministic\":\n",
    "                                    key_ = generate_deterministic_key(f\"{user_}_{datetime.datetime.now()}\")\n",
    "                                    rule_[\"encrypt\"]=f\"{key_}:::Deterministic\"\n",
    "                                elif rule_[\"encrypt\"] == \"Non-Deterministic\":\n",
    "                                    key_ = generate_non_deterministic_key()\n",
    "                                    # print(type(key_))\n",
    "                                    rule_[\"encrypt\"]=f\"{key_}:::Non-Deterministic\"\n",
    "                                # print(rule_)\n",
    "                                #write a log for encryption \n",
    "                                pd.DataFrame([[user_,DATABASE_,table_,column_,rule_[\"encrypt\"].split(\":::\")[0],rule_[\"encrypt\"].split(\":::\")[1],datetime.datetime.now()]],columns=[\"user\",\"database\",\"table\",\"column\",\"key\",\"type\",\"time\"]).to_csv(\"Encrypt_log.csv\",mode=\"a\",index=False)\n",
    "                            \n",
    "                            #apply given rules from config to data\n",
    "                            data_df[column_] = data_df[column_].apply(lambda x : anonymize(str(x),rule_,tag_))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        rule_ = conf_[user_][\"Columns_for_custom_anonymise\"][column_]\n",
    "                        #debug for checking values\n",
    "                        # print(user_,column_,tag_,rule_)\n",
    "                        #check if rules if encryption is asked to register keys and user into log for security purpose\n",
    "                        if [x for x in rule_.keys()][0] == \"encrypt\":\n",
    "                            # check if its deterministic or non deterministic type of encryption as both requires different keys\n",
    "                            #debug\n",
    "                            # print(\"yes\")\n",
    "                            if rule_[\"encrypt\"] == \"Deterministic\":\n",
    "                                key_ = generate_deterministic_key(f\"{user_}_{datetime.datetime.now()}\")\n",
    "                                rule_[\"encrypt\"]=f\"{key_}:::Deterministic\"\n",
    "                            elif rule_[\"encrypt\"] == \"Non-Deterministic\":\n",
    "                                key_ = generate_non_deterministic_key()\n",
    "                                # print(type(key_))\n",
    "                                rule_[\"encrypt\"]=f\"{key_}:::Non-Deterministic\"\n",
    "                            # print(rule_)\n",
    "                            #write a log for encryption \n",
    "                            key,type_ = rule_[\"encrypt\"].split(\":::\")\n",
    "                            pd.DataFrame([[user_,DATABASE_,table_,column_,key,type_,datetime.datetime.now()]],columns=[\"user\",\"database\",\"table\",\"column\",\"key\",\"type\",\"time\"]).to_csv(\"Encrypt_log.csv\",mode=\"a\",index=False)\n",
    "                        \n",
    "                        #apply given rules from config to data\n",
    "                        data_df[column_] = data_df[column_].apply(lambda x : anonymize(str(x),rule_,tag_))\n",
    "                  \n",
    "            #write data into csv      \n",
    "            data_df.to_csv(f\"{user_}_{table_}_{datetime.datetime.now()}.csv\",index=False)\n",
    "            print(f\"table {table_} created\")  \n",
    "        else:\n",
    "            print(f\"user {user_} has no access table {table_}\")\n",
    "    pd.DataFrame(dq_table_lst).to_csv(f\"{user_}_tables_quality.csv\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DContracts_DQP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
