{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.inHousedqchecks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manonymiser\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manonymiser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m anonymize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparksession\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_spark_utility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_mssql_sparkContext\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdq_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable_level_intial_assessment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tablelevel_asessment_stats\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\Utils\\dq_utils\\table_level_intial_assessment.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_connectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdg_otherUtilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdq_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqsensors_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\Utils\\dq_utils\\dqsensors_table.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minHousedqchecks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqsensors_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Table-Level Sensors\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Provides the total number of columns in dataframe\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_number_of_columns\u001b[39m(df):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src.inHousedqchecks'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from src.Utils.HelperFunction.helper import fetchDataMssql,encrypt_data,generate_deterministic_key,generate_non_deterministic_key\n",
    "from src.Utils.connectors.connector import connectMssql\n",
    "import json\n",
    "import re ,ast\n",
    "from src.Utils.anonymiser.anonymiser import anonymize\n",
    "from Utils.sparksession.local_spark_utility import get_mssql_sparkContext\n",
    "from Utils.dq_utils.table_level_intial_assessment import get_tablelevel_asessment_stats\n",
    "import numpy as np\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_mssql_sparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"\\Users\\dhanush.shetty\\DContracts_DQP\\src\\config\\config.json\",\"r\") as j:\n",
    "    conf_=json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_= pd.read_csv(r\"C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\access_control\\Access_table.csv\")\n",
    "  \n",
    "  #asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(r\"C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\metadata\\Metadata_mssql_adventureworks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df=pd.read_csv(r\"C:\\Users\\dhanush.shetty\\presidio_demo\\data\\Classified_metadata.csv\")\n",
    "# Convert classified metadata df to spark df\n",
    "classified_df_spark = spark.createDataFrame(classified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_ in conf_:\n",
    "    SERVER_=conf_[user_].get(\"SERVER_NAME\")\n",
    "    DATABASE_=conf_[user_].get(\"DATABASE_NAME\")\n",
    "    DATABASE_IP=conf_[user_].get(\"DATABASE_IP\")\n",
    "    SCHEMA_=conf_[user_].get(\"SCHEMA_NAME\")\n",
    "    PASSWORD_=conf_[user_].get(\"PASSWORD\")\n",
    "    #get tables\n",
    "    for table_ in conf_[user_].get(\"TABLE_REQUIRED\"):\n",
    "        if user_ in str(access_[\"USER_ACCESS\"][access_[\"Table Name\"]==table_].values):\n",
    "            #fetch data into df\n",
    "            #save data to data_df variable\n",
    "            #run table level score\n",
    "            data_df, table_assessment_dict = get_tablelevel_asessment_stats(spark=spark, table_name=table_, database_name=DATABASE_, database_ip=DATABASE_IP, metadata_df=classified_df_spark)\n",
    "            \n",
    "            #run column level score\n",
    "            column_assessment_list = get_tablelevel_asessment_stats(spark=spark, table_name=table_, database_name=DATABASE_, database_ip=DATABASE_IP, metadata_df=classified_df_spark)\n",
    "            \n",
    "            columns_to_retain = conf_[user_][\"Columns_to_retain\"]\n",
    "            columns_to_discard = conf_[user_][\"Columns_to_discard\"]\n",
    "            columns_to_custom_anonymise = list(conf_[user_][\"Columns_for_custom_anonymise\"].keys())\n",
    "            #iterate column and tag from classified data\n",
    "            for column_,tag_ in classified_df[[\"Column Name\",\"Tag\"]][((classified_df[\"Table Name\"]==table_)\\\n",
    "                                                                     & \\\n",
    "                                                                       ((classified_df[\"Pii\"]==\"Yes\") \\\n",
    "                                                                        | (classified_df[\"Column Name\"].isin(columns_to_retain) \\\n",
    "                                                                        | (classified_df[\"Column Name\"].isin(columns_to_custom_anonymise) )))\\\n",
    "                                                                     & \\\n",
    "                                                                        (~classified_df[\"Column Name\"].isin(columns_to_discard)))\\\n",
    "                                                                    ].values:\n",
    "                \n",
    "                    #seperate rules from config \n",
    "                    if column_ not in  columns_to_custom_anonymise :\n",
    "                        #skip column with no tags\n",
    "                        if str(tag_) != \"nan\":\n",
    "                            rule_ = conf_[user_].get(f\"{tag_}\")\n",
    "                            #debug for checking values\n",
    "                            # print(user_,column_,tag_,rule_)\n",
    "                            #check if rules if encryption is asked to register keys and user into log for security purpose\n",
    "                            if [x for x in rule_.keys()][0] == \"encrypt\":\n",
    "                                # check if its deterministic or non deterministic type of encryption as both requires different keys\n",
    "                                #debug\n",
    "                                # print(\"yes\")\n",
    "                                if rule_[\"encrypt\"] == \"Deterministic\":\n",
    "                                    key_ = generate_deterministic_key(f\"{user_}_{datetime.datetime.now()}\")\n",
    "                                    rule_[\"encrypt\"]=f\"{key_}:::Deterministic\"\n",
    "                                elif rule_[\"encrypt\"] == \"Non-Deterministic\":\n",
    "                                    key_ = generate_non_deterministic_key()\n",
    "                                    # print(type(key_))\n",
    "                                    rule_[\"encrypt\"]=f\"{key_}:::Non-Deterministic\"\n",
    "                                # print(rule_)\n",
    "                                #write a log for encryption \n",
    "                                pd.DataFrame([[user_,DATABASE_,table_,column_,rule_[\"encrypt\"].split(\":::\")[0],rule_[\"encrypt\"].split(\":::\")[1],datetime.datetime.now()]],columns=[\"user\",\"database\",\"table\",\"column\",\"key\",\"type\",\"time\"]).to_csv(\"Encrypt_log.csv\",mode=\"a\",index=False)\n",
    "                            \n",
    "                            #apply given rules from config to data\n",
    "                            data_df[column_] = data_df[column_].apply(lambda x : anonymize(str(x),rule_,tag_))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        rule_ = conf_[user_][\"Columns_for_custom_anonymise\"][column_]\n",
    "                        #debug for checking values\n",
    "                        # print(user_,column_,tag_,rule_)\n",
    "                        #check if rules if encryption is asked to register keys and user into log for security purpose\n",
    "                        if [x for x in rule_.keys()][0] == \"encrypt\":\n",
    "                            # check if its deterministic or non deterministic type of encryption as both requires different keys\n",
    "                            #debug\n",
    "                            # print(\"yes\")\n",
    "                            if rule_[\"encrypt\"] == \"Deterministic\":\n",
    "                                key_ = generate_deterministic_key(f\"{user_}_{datetime.datetime.now()}\")\n",
    "                                rule_[\"encrypt\"]=f\"{key_}:::Deterministic\"\n",
    "                            elif rule_[\"encrypt\"] == \"Non-Deterministic\":\n",
    "                                key_ = generate_non_deterministic_key()\n",
    "                                # print(type(key_))\n",
    "                                rule_[\"encrypt\"]=f\"{key_}:::Non-Deterministic\"\n",
    "                            # print(rule_)\n",
    "                            #write a log for encryption \n",
    "                            key,type_ = rule_[\"encrypt\"].split(\":::\")\n",
    "                            pd.DataFrame([[user_,DATABASE_,table_,column_,key,type_,datetime.datetime.now()]],columns=[\"user\",\"database\",\"table\",\"column\",\"key\",\"type\",\"time\"]).to_csv(\"Encrypt_log.csv\",mode=\"a\",index=False)\n",
    "                        \n",
    "                        #apply given rules from config to data\n",
    "                        data_df[column_] = data_df[column_].apply(lambda x : anonymize(str(x),rule_,tag_))\n",
    "                  \n",
    "            #write data into csv      \n",
    "            data_df.to_csv(f\"{user_}_{table_}_{datetime.datetime.now()}.csv\",index=False)\n",
    "            print(f\"table {table_} created\")  \n",
    "        else:\n",
    "            print(f\"user {user_} has no access table {table_}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DContracts_DQP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
