{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msa\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHelperFunction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetchDataMssql,encrypt_data,generate_deterministic_key,generate_non_deterministic_key\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUtils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m connectMssql\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\Utils\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHelperFunction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGenAiClassifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenaiPIIClassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\Utils\\HelperFunction\\helper.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyodbc\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# from datetime import datetime\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# To Read Excel into spark\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_non_deterministic_key\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\sqlalchemy\\orm\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exc \u001b[38;5;28;01mas\u001b[39;00m exc\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mapper \u001b[38;5;28;01mas\u001b[39;00m mapperlib\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m strategy_options \u001b[38;5;28;01mas\u001b[39;00m strategy_options\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_orm_constructors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mapper_fn \u001b[38;5;28;01mas\u001b[39;00m mapper\n",
      "File \u001b[1;32mc:\\Users\\dhanush.shetty\\DContracts_DQP\\lib\\site-packages\\sqlalchemy\\orm\\mapper.py:50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exc \u001b[38;5;28;01mas\u001b[39;00m orm_exc\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instrumentation\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loading\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m properties\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util \u001b[38;5;28;01mas\u001b[39;00m orm_util\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1349\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1321\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:101\u001b[0m, in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:93\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:87\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from src.Utils.HelperFunction.helper import fetchDataMssql,encrypt_data,generate_deterministic_key,generate_non_deterministic_key\n",
    "from src.Utils.connectors.connector import connectMssql\n",
    "import json\n",
    "import re ,ast\n",
    "from src.Utils.anonymiser.anonymiser import anonymize\n",
    "from Utils.sparksession.local_spark_utility import get_mssql_sparkContext\n",
    "from Utils.dq_utils.table_level_intial_assessment import get_tablelevel_asessment_stats\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_mssql_sparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"\\Users\\dhanush.shetty\\DContracts_DQP\\src\\config\\config.json\",\"r\") as j:\n",
    "    conf_=json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_= pd.read_csv(r\"C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\access_control\\Access_table.csv\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(r\"C:\\Users\\dhanush.shetty\\DContracts_DQP\\src\\metadata\\Metadata_mssql_adventureworks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df=pd.read_csv(r\"C:\\Users\\dhanush.shetty\\presidio_demo\\data\\Classified_metadata.csv\")\n",
    "# Convert classified metadata df to spark df\n",
    "classified_df_spark = spark.createDataFrame(classified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_ in conf_:\n",
    "    SERVER_=conf_[user_].get(\"SERVER_NAME\")\n",
    "    DATABASE_=conf_[user_].get(\"DATABASE_NAME\")\n",
    "    DATABASE_IP=conf_[user_].get(\"DATABASE_IP\")\n",
    "    SCHEMA_=conf_[user_].get(\"SCHEMA_NAME\")\n",
    "    PASSWORD_=conf_[user_].get(\"PASSWORD\")\n",
    "    #get tables\n",
    "    for table_ in conf_[user_].get(\"TABLE_REQUIRED\"):\n",
    "        if user_ in str(access_[\"USER_ACCESS\"][access_[\"Table Name\"]==table_].values):\n",
    "            #fetch data into df\n",
    "            #save data to data_df variable\n",
    "            #run table level score\n",
    "            data_df, table_assessment_dict = get_tablelevel_asessment_stats(spark=spark, table_name=table_, database_name=DATABASE_, database_ip=DATABASE_IP, metadata_df=classified_df_spark)\n",
    "            \n",
    "            #run column level score\n",
    "            column_assessment_list = get_tablelevel_asessment_stats(spark=spark, table_name=table_, database_name=DATABASE_, database_ip=DATABASE_IP, metadata_df=classified_df_spark)\n",
    "            \n",
    "            columns_to_retain = conf_[user_][\"Columns_to_retain\"]\n",
    "            columns_to_discard = conf_[user_][\"Columns_to_discard\"]\n",
    "            columns_to_custom_anonymise = list(conf_[user_][\"Columns_for_custom_anonymise\"].keys())\n",
    "            #iterate column and tag from classified data\n",
    "            for column_,tag_ in classified_df[[\"Column Name\",\"Tag\"]][((classified_df[\"Table Name\"]==table_)\\\n",
    "                                                                     & \\\n",
    "                                                                       ((classified_df[\"Pii\"]==\"Yes\") \\\n",
    "                                                                        | (classified_df[\"Column Name\"].isin(columns_to_retain) \\\n",
    "                                                                        | (classified_df[\"Column Name\"].isin(columns_to_custom_anonymise) )))\\\n",
    "                                                                     & \\\n",
    "                                                                        (~classified_df[\"Column Name\"].isin(columns_to_discard)))\\\n",
    "                                                                    ].values:\n",
    "                \n",
    "                    #seperate rules from config \n",
    "                    if column_ not in  columns_to_custom_anonymise :\n",
    "                        #skip column with no tags\n",
    "                        if str(tag_) != \"nan\":\n",
    "                            rule_ = conf_[user_].get(f\"{tag_}\")\n",
    "                            #debug for checking values\n",
    "                            # print(user_,column_,tag_,rule_)\n",
    "                            #check if rules if encryption is asked to register keys and user into log for security purpose\n",
    "                            if [x for x in rule_.keys()][0] == \"encrypt\":\n",
    "                                # check if its deterministic or non deterministic type of encryption as both requires different keys\n",
    "                                #debug\n",
    "                                # print(\"yes\")\n",
    "                                if rule_[\"encrypt\"] == \"Deterministic\":\n",
    "                                    key_ = generate_deterministic_key(f\"{user_}_{datetime.datetime.now()}\")\n",
    "                                    rule_[\"encrypt\"]=f\"{key_}:::Deterministic\"\n",
    "                                elif rule_[\"encrypt\"] == \"Non-Deterministic\":\n",
    "                                    key_ = generate_non_deterministic_key()\n",
    "                                    # print(type(key_))\n",
    "                                    rule_[\"encrypt\"]=f\"{key_}:::Non-Deterministic\"\n",
    "                                # print(rule_)\n",
    "                                #write a log for encryption \n",
    "                                pd.DataFrame([[user_,DATABASE_,table_,column_,rule_[\"encrypt\"].split(\":::\")[0],rule_[\"encrypt\"].split(\":::\")[1],datetime.datetime.now()]],columns=[\"user\",\"database\",\"table\",\"column\",\"key\",\"type\",\"time\"]).to_csv(\"Encrypt_log.csv\",mode=\"a\",index=False)\n",
    "                            \n",
    "                            #apply given rules from config to data\n",
    "                            data_df[column_] = data_df[column_].apply(lambda x : anonymize(str(x),rule_,tag_))\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        rule_ = conf_[user_][\"Columns_for_custom_anonymise\"][column_]\n",
    "                        #debug for checking values\n",
    "                        # print(user_,column_,tag_,rule_)\n",
    "                        #check if rules if encryption is asked to register keys and user into log for security purpose\n",
    "                        if [x for x in rule_.keys()][0] == \"encrypt\":\n",
    "                            # check if its deterministic or non deterministic type of encryption as both requires different keys\n",
    "                            #debug\n",
    "                            # print(\"yes\")\n",
    "                            if rule_[\"encrypt\"] == \"Deterministic\":\n",
    "                                key_ = generate_deterministic_key(f\"{user_}_{datetime.datetime.now()}\")\n",
    "                                rule_[\"encrypt\"]=f\"{key_}:::Deterministic\"\n",
    "                            elif rule_[\"encrypt\"] == \"Non-Deterministic\":\n",
    "                                key_ = generate_non_deterministic_key()\n",
    "                                # print(type(key_))\n",
    "                                rule_[\"encrypt\"]=f\"{key_}:::Non-Deterministic\"\n",
    "                            # print(rule_)\n",
    "                            #write a log for encryption \n",
    "                            key,type_ = rule_[\"encrypt\"].split(\":::\")\n",
    "                            pd.DataFrame([[user_,DATABASE_,table_,column_,key,type_,datetime.datetime.now()]],columns=[\"user\",\"database\",\"table\",\"column\",\"key\",\"type\",\"time\"]).to_csv(\"Encrypt_log.csv\",mode=\"a\",index=False)\n",
    "                        \n",
    "                        #apply given rules from config to data\n",
    "                        data_df[column_] = data_df[column_].apply(lambda x : anonymize(str(x),rule_,tag_))\n",
    "                  \n",
    "            #write data into csv      \n",
    "            data_df.to_csv(f\"{user_}_{table_}_{datetime.datetime.now()}.csv\",index=False)\n",
    "            print(f\"table {table_} created\")  \n",
    "        else:\n",
    "            print(f\"user {user_} has no access table {table_}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DContracts_DQP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
